{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8x7Xd_Yf0Dst",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "103a5278-39e6-4dbc-e46a-0dbf95a7d415"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOverfitting\\n\\nOverfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset.\\nBecause of this, the model starts caching noise and inaccurate values present in the dataset,\\nand all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.\\n\\nThe chances of occurrence of overfitting increase as much we provide training to our model.\\nIt means the more we train our model, the more chances of occurring the overfitted model.\\n\\nOverfitting is the main problem that occurs in supervised learning.\\n\\nUnderfitting\\n\\nUnderfitting occurs when our machine learning model is not able to capture the underlying trend of the data.\\nTo avoid the overfitting in the model, the fed of training data can be stopped at an early stage,\\ndue to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\\n\\nIn the case of underfitting, the model is not able to learn enough from the training data,\\nand hence it reduces the accuracy and produces unreliable predictions.\\n\\nAn underfitted model has high bias and low variance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
        "\n",
        "'''\n",
        "Overfitting\n",
        "\n",
        "Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset.\n",
        "Because of this, the model starts caching noise and inaccurate values present in the dataset,\n",
        "and all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.\n",
        "\n",
        "The chances of occurrence of overfitting increase as much we provide training to our model.\n",
        "It means the more we train our model, the more chances of occurring the overfitted model.\n",
        "\n",
        "Overfitting is the main problem that occurs in supervised learning.\n",
        "\n",
        "Underfitting\n",
        "\n",
        "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data.\n",
        "To avoid the overfitting in the model, the fed of training data can be stopped at an early stage,\n",
        "due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
        "\n",
        "In the case of underfitting, the model is not able to learn enough from the training data,\n",
        "and hence it reduces the accuracy and produces unreliable predictions.\n",
        "\n",
        "An underfitted model has high bias and low variance.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2: How can we reduce overfitting? Explain in brief.\n",
        "\n",
        "'''\n",
        "Both overfitting and underfitting cause the degraded performance of the machine learning model.\n",
        "But the main cause is overfitting, so there are some ways by which we can reduce the occurrence of overfitting in our model.\n",
        "\n",
        "Cross-Validation\n",
        "Training with more data\n",
        "Removing features\n",
        "Early stopping the training\n",
        "Regularization\n",
        "Ensembling\n",
        "\n",
        "By spliting the whole_dataset into traing-dataset and testing_dataset'''"
      ],
      "metadata": {
        "id": "5TUO4K6s1NpG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f0d89fda-5820-4f72-99d9-8fc8fd53e5db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBoth overfitting and underfitting cause the degraded performance of the machine learning model.\\nBut the main cause is overfitting, so there are some ways by which we can reduce the occurrence of overfitting in our model.\\n\\nCross-Validation\\nTraining with more data\\nRemoving features\\nEarly stopping the training\\nRegularization\\nEnsembling\\n\\nBy spliting the whole_dataset into traing-dataset and testing_dataset'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
        "\n",
        "'''\n",
        "Underfitting in Machine Learning\n",
        "Its occurrence simply means that our model or the algorithm does not fit the data well enough.\n",
        "It usually happens when we have less data to build an accurate model and also when we try to build a linear model with fewer non-linear data\n",
        "1. BIAS\n",
        "2. REMOVING Important feature\n",
        "'''"
      ],
      "metadata": {
        "id": "L0YYnxES1mlG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "55e64c4c-8016-46d3-c125-4f1e1b2786ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUnderfitting in Machine Learning\\nIts occurrence simply means that our model or the algorithm does not fit the data well enough.\\nIt usually happens when we have less data to build an accurate model and also when we try to build a linear model with fewer non-linear data\\n1. BIAS\\n2. REMOVING Important feature\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
        "'''\n",
        "Bias Variance Tradeoff\n",
        "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone.\n",
        "If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias.\n",
        "\n",
        "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance.\n",
        "\n",
        "When a data engineer modifies the ML algorithm to better fit a given data set,\n",
        "it will lead to low bias—but it will increase variance. This way, the model will fit with the data set while increasing the chances of inaccurate predictions.\n",
        "\n",
        "The same applies when creating a low variance model with a higher bias.\n",
        "While it will reduce the risk of inaccurate predictions, the model will not properly match the data set.\n",
        "\n",
        "It’s a delicate balance between these bias and variance. Importantly,\n",
        "however, having a higher variance does not indicate a bad ML algorithm. Machine learning algorithms should be able to handle some variance.'''"
      ],
      "metadata": {
        "id": "rEKvGMB012Tm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "83f2230c-73e2-4f06-e2b8-13b40b063c83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBias Variance Tradeoff\\nIf the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone.\\nIf algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias.\\n\\nBias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance.\\n\\nWhen a data engineer modifies the ML algorithm to better fit a given data set,\\nit will lead to low bias—but it will increase variance. This way, the model will fit with the data set while increasing the chances of inaccurate predictions.\\n\\nThe same applies when creating a low variance model with a higher bias.\\nWhile it will reduce the risk of inaccurate predictions, the model will not properly match the data set.\\n\\nIt’s a delicate balance between these bias and variance. Importantly,\\nhowever, having a higher variance does not indicate a bad ML algorithm. Machine learning algorithms should be able to handle some variance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.How can you determine whether your model is overfitting or underfitting?\n",
        "\n",
        "'''\n",
        "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data.\n",
        "Your model is underfitting the training data when the model performs poorly on the training data.'''"
      ],
      "metadata": {
        "id": "6pxitPzy24n_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "05ce9fe9-7ac2-4701-9afb-c43b094afded"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWe can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data.\\nYour model is underfitting the training data when the model performs poorly on the training data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
        "\n",
        "'''\n",
        "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data.\n",
        "In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data.\n",
        "\n",
        "The trade-off challenge depends on the type of model under consideration. A linear machine-learning algorithm will exhibit high bias but low variance.\n",
        "On the other hand, a non-linear algorithm will exhibit low bias but high variance.\n",
        "Using a linear model with a data set that is non-linear will introduce bias into the model.\n",
        "The model will underfit the target functions compared to the training data set.\n",
        "The reverse is true as well — if you use a non-linear model on a linear dataset, the non-linear model will overfit the target function.\n",
        "\n",
        "To deal with these trade-off challenges, a data scientist must build a learning algorithm flexible enough to correctly fit the data.\n",
        "However, if the algorithm has too much flexibility built in, it may be too linear and provide results with a high variance from each training data set.'''"
      ],
      "metadata": {
        "id": "vVLnPZeoJmSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "244e1bf5-3353-4b3f-9d4f-46849962c2fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nA model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data.\\nIn comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data.\\n\\nThe trade-off challenge depends on the type of model under consideration. A linear machine-learning algorithm will exhibit high bias but low variance.\\nOn the other hand, a non-linear algorithm will exhibit low bias but high variance.\\nUsing a linear model with a data set that is non-linear will introduce bias into the model.\\nThe model will underfit the target functions compared to the training data set.\\nThe reverse is true as well — if you use a non-linear model on a linear dataset, the non-linear model will overfit the target function.\\n\\nTo deal with these trade-off challenges, a data scientist must build a learning algorithm flexible enough to correctly fit the data.\\nHowever, if the algorithm has too much flexibility built in, it may be too linear and provide results with a high variance from each training data set.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
        "\n",
        "'''\n",
        "n short, Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes,\n",
        "or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model,\n",
        "avoiding the risk of Overfitting.\n",
        "\n",
        "Techniques of Regularization\n",
        "There are mainly two types of regularization techniques, which are given below:\n",
        "Ridge Regression\n",
        "Lasso Regression\n",
        "\n",
        "Ridge Regression\n",
        "\n",
        "Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions.\n",
        "Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.\n",
        "In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\n",
        "The equation for the cost function in ridge regression will be:\n",
        "Regularization in Machine Learning\n",
        "In the above equation, the penalty term regularizes the coefficients of the model, and hence ridge regression reduces the amplitudes of the coefficients that decreases the complexity of the model.\n",
        "As we can see from the above equation, if the values of λ tend to zero, the equation becomes the cost function of the linear regression model. Hence, for the minimum value of λ, the model will resemble the linear regression model.\n",
        "A general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used.\n",
        "It helps to solve the problems if we have more parameters than samples.\n",
        "\n",
        "Lasso Regression:\n",
        "\n",
        "Lasso regression is another regularization technique to reduce the complexity of the model. It stands for Least Absolute and Selection Operator.\n",
        "It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.\n",
        "Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\n",
        "It is also called as L1 regularization. The equation for the cost function of Lasso regression will be:\n",
        "Regularization in Machine Learning\n",
        "Some of the features in this technique are completely neglected for model evaluation.\n",
        "Hence, the Lasso regression can help us to reduce the overfitting in the model as well as the feature selection.'''\n",
        "\n"
      ],
      "metadata": {
        "id": "iFgNLXp0J9il",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "53b170d1-3b1b-4748-b210-3a44a544278c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nn short, Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, \\nor shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, \\navoiding the risk of Overfitting.\\n\\nTechniques of Regularization\\nThere are mainly two types of regularization techniques, which are given below:\\nRidge Regression\\nLasso Regression\\n\\nRidge Regression\\n\\nRidge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions.\\nRidge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.\\nIn this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\\nThe equation for the cost function in ridge regression will be:\\nRegularization in Machine Learning\\nIn the above equation, the penalty term regularizes the coefficients of the model, and hence ridge regression reduces the amplitudes of the coefficients that decreases the complexity of the model.\\nAs we can see from the above equation, if the values of λ tend to zero, the equation becomes the cost function of the linear regression model. Hence, for the minimum value of λ, the model will resemble the linear regression model.\\nA general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used.\\nIt helps to solve the problems if we have more parameters than samples.\\n\\nLasso Regression:\\n\\nLasso regression is another regularization technique to reduce the complexity of the model. It stands for Least Absolute and Selection Operator.\\nIt is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.\\nSince it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\\nIt is also called as L1 regularization. The equation for the cost function of Lasso regression will be:\\nRegularization in Machine Learning\\nSome of the features in this technique are completely neglected for model evaluation.\\nHence, the Lasso regression can help us to reduce the overfitting in the model as well as the feature selection.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}